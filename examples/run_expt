#!/bin/bash -e

export PYTHONPATH=../dca_plda:$PYTHONPATH

# Defaults
train_set=vox+fvcaus
dev_sets="sitw.dev"
arch_config=configs/arch/dplda.ini
seed=0
device=0
root_dir=output

while getopts "t:d:o:a:s:D:" flag; do
    case ${flag} in
	t ) train_set=$OPTARG;;
	d ) dev_sets=$OPTARG;;
	o ) root_dir=$OPTARG;;
	a ) arch_config=$OPTARG;;
	s ) seed=$OPTARG;;
	D ) device=$OPTARG;;
	\? ) echo "Invalid option -$OPTARG"; exit 1;;
    esac
done


###### Location of inputs

data_dir=./data
trn_emb=$data_dir/train/embeddings.h5
trn_meta=$data_dir/train/metadata_$train_set
ini_meta=$data_dir/train/metadata_$train_set
train_config1=configs/train/stage1.ini
train_config2=configs/train/stage2.ini

###### Training in two stages

train_flags="--restart --seed $seed"
if [ $device != 'CPU' ]; then
    train_flags="$train_flags --cuda"
    export CUDA_VISIBLE_DEVICES=$device
else
    export CUDA_VISIBLE_DEVICES=""
fi

out_dir=$root_dir/
mkdir -p $out_dir/stage1/models
dev_table=$out_dir/stage1/models/dev_table

for dev in $dev_sets; do
    dev_emb=$data_dir/eval/$dev/embeddings.h5
    dev_key=$data_dir/eval/$dev/keys/all.h5
    dev_emap=$data_dir/eval/$dev/enroll.lst
    dev_tmap=$data_dir/eval/$dev/test.lst
    dev_durs=$data_dir/eval/$dev/durations
    echo $dev $dev_emb $dev_key $dev_emap $dev_tmap $dev_durs 
done > $dev_table
date=`date +%Y.%m.%d.%H.%M`

if [ ! -f $out_dir/stage1/models/DONE ]; then
    echo "Starting stage1 in $out_dir/stage1/models"
    echo "Log file: $out_dir/stage1/models/log.$date"
    python -u ../scripts/train.py $train_flags --configs $train_config1,$arch_config --init_subset $ini_meta $trn_emb $trn_meta $dev_table $out_dir/stage1/models > $out_dir/stage1/models/log.$date
fi

if [ ! -f $out_dir/stage2/models/DONE ] && [ $train_config2 != "" ]; then
    echo "Starting stage1 in $out_dir/stage2/models"
    echo "Log file: $out_dir/stage2/models/log.$date"
    # Copy the last model from stage1 in the outdir for stage2. Since we set --restart, the training script will look for
    # existing models and load it up.
    last_model_from_first_stage=`ls $out_dir/stage1/models/last_*`
    dev_loss=`echo $last_model_from_first_stage | sed -e 's,.*devloss_,,' -e 's,_.*,,' -e 's,\.pth,,'`
    mkdir -p $out_dir/stage2/models
    cp $last_model_from_first_stage $out_dir/stage2/models/model_epoch_0000_devloss_$dev_loss.pth
    python -u ../scripts/train.py $train_flags --configs $train_config1,$train_config2,$arch_config $trn_emb $trn_meta $dev_table $out_dir/stage2/models > $out_dir/stage2/models/log.$date
fi

###### Evaluation

for stage in stage1 stage2; do

    # Evaluate the first and the best model in each stage
    best_model=`ls $out_dir/$stage/models/best*.pth`
    first_model=`ls $out_dir/$stage/models/first*.pth`

    for model in $best_model $first_model; do

	mname=`echo $model | sed -e 's,.*epoch_,ep,' -e 's,_.*,,'`
	dir=$out_dir/$stage/eval_$mname
	
	for evalset in sitw.dev sitw.eval heldout_fvcaus voxceleb2; do

	    emb=$data_dir/eval/$evalset/embeddings.h5
	    keys=$data_dir/eval/$evalset/key.lst
	    emap=$data_dir/eval/$evalset/enroll.lst
	    tmap=$data_dir/eval/$evalset/test.lst
	    durs=$data_dir/eval/$evalset/durations
	
	    eval_dir=$dir/$evalset
	    mkdir -p $eval_dir
	    
	    if [ ! -f $eval_dir/results ]; then
		echo "#######################################################################"
		echo "Testing $evalset with model $model"
		# All wavs with less than 2seconds of speech are discarded from scoring. Their score
		# will be considered 0 when computing the performance.
		gawk '$2>2' $durs > $eval_dir/durations
		python3 ../scripts/eval.py --durs $eval_dir/durations --keylist $keys --min_dur 2 --set $evalset $model $emb $emap $tmap $eval_dir 
	    fi
	done

	if [ $model == $best_model ]; then
	    ln -fs eval_$mname $out_dir/$stage/eval_best
	fi

	cat $dir/*/results | head -2 > $dir/all_results
	cat $dir/*/results | sort | fgrep ":" >> $dir/all_results

    done

done

# Delete all models except the first, best and last, which are saved in a copy with specific names
find $out_dir/ -name "model_epoch*" | xargs rm -f

